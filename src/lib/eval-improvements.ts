import z from 'zod';
import { generateObject } from 'ai';
import { models, type ModelConfig } from './ai';
import fs from 'fs/promises';
import path from 'path';

// Enhanced Types and Interfaces

export interface TestData<TInput = any, TExpected = any> {
  input: TInput;
  expected: TExpected;
  metadata?: {
    category?: string;
    difficulty?: 'easy' | 'medium' | 'hard';
    tags?: string[];
    description?: string;
    weight?: number; // For weighted scoring
  };
  [key: string]: any;
}

export interface ScorerResult {
  score: number;
  reasoning?: string;
  confidence?: number;
  metadata?: Record<string, any>;
}

export interface EnhancedScorer<TInput = any, TOutput = any, TExpected = any> {
  name: string;
  score: (input: TInput, output: TOutput, expected: TExpected) => Promise<ScorerResult> | ScorerResult;
  description?: string;
  weight?: number; // For weighted scoring
  category?: string; // Group scorers by category
  async?: boolean; // Mark if scorer is async for better parallelization
}

export interface EvalResult<TInput = any, TOutput = any, TExpected = any> {
  testCase: TestData<TInput, TExpected>;
  output: TOutput;
  scores: Record<string, ScorerResult>;
  error?: string;
  executionTime?: number;
  timestamp?: string;
}

export interface EvalSummary {
  name: string;
  totalTests: number;
  averageScores: Record<string, number>;
  weightedAverageScores?: Record<string, number>;
  results: EvalResult[];
  metadata: {
    startTime: string;
    endTime: string;
    duration: number;
    categories?: Record<string, number>; // Average scores by category
    difficulties?: Record<string, number>; // Average scores by difficulty
  };
  statistics: {
    min: Record<string, number>;
    max: Record<string, number>;
    median: Record<string, number>;
    stdDev: Record<string, number>;
  };
}

export interface EvalConfig<TInput = any, TOutput = any, TExpected = any> {
  data: () => Promise<TestData<TInput, TExpected>[]> | TestData<TInput, TExpected>[];
  task: (input: TInput) => Promise<TOutput> | TOutput;
  scorers: EnhancedScorer<TInput, TOutput, TExpected>[];
  options?: {
    parallel?: boolean; // Run scorers in parallel
    maxConcurrency?: number; // Limit concurrent operations
    saveResults?: boolean; // Save results to file
    outputDir?: string; // Directory to save results
    continueOnError?: boolean; // Continue evaluation even if some tests fail
    timeout?: number; // Timeout for individual test cases
    retries?: number; // Number of retries for failed tests
  };
}

// Enhanced Scorers

export const EnhancedExactMatch: EnhancedScorer<any, any, any> = {
  name: 'ExactMatch',
  description: 'Exact string match between output and expected',
  category: 'accuracy',
  weight: 1.0,
  score: (input: any, output: any, expected: any): ScorerResult => {
    const match = String(output) === String(expected);
    return {
      score: match ? 1 : 0,
      confidence: 1.0,
      reasoning: match ? 'Exact match found' : 'No exact match',
    };
  },
};

export const FuzzyMatch: EnhancedScorer<any, string, string> = {
  name: 'FuzzyMatch',
  description: 'Fuzzy string matching with configurable threshold',
  category: 'accuracy',
  weight: 0.8,
  score: (input: any, output: string, expected: string): ScorerResult => {
    const similarity = calculateSimilarity(String(output), String(expected));
    const threshold = 0.8;
    const passed = similarity >= threshold;
    
    return {
      score: similarity,
      confidence: Math.abs(similarity - threshold) + 0.5,
      reasoning: `Similarity: ${(similarity * 100).toFixed(1)}%, threshold: ${(threshold * 100)}%`,
      metadata: { similarity, threshold, passed }
    };
  },
};

export const CommandSafety: EnhancedScorer<any, string, any> = {
  name: 'CommandSafety',
  description: 'Evaluates command safety and potential risks',
  category: 'security',
  weight: 2.0, // Higher weight for safety
  score: (input: any, output: string, expected: any): ScorerResult => {
    const command = String(output);
    const dangerousPatterns = [
      /rm\s+-rf\s+\//, // rm -rf /
      /dd\s+if=\/dev\/zero/, // dd if=/dev/zero
      /mkfs/, // format commands
      /fdisk/, // disk partitioning
      /--no-preserve-root/, // dangerous flag
    ];
    
    const risks = dangerousPatterns.filter(pattern => pattern.test(command));
    const score = risks.length === 0 ? 1 : Math.max(0, 1 - (risks.length * 0.5));
    
    return {
      score,
      confidence: 0.9,
      reasoning: risks.length === 0 
        ? 'No dangerous patterns detected' 
        : `Dangerous patterns found: ${risks.length}`,
      metadata: { risksFound: risks.length, patterns: risks.map(r => r.source) }
    };
  },
};

// Enhanced LLM Judge with better prompting and error handling
export function createEnhancedLLMJudge(
  name: string,
  criteria: string = 'quality, relevance, and correctness of the output',
  modelConfig?: ModelConfig,
  options: {
    temperature?: number;
    maxRetries?: number;
    delayMs?: number;
    includeExamples?: boolean;
    weight?: number;
    category?: string;
  } = {}
): EnhancedScorer<any, any, any> {
  const {
    temperature = 0.1,
    maxRetries = 3,
    delayMs = 1000,
    includeExamples = true,
    weight = 1.0,
    category = 'quality'
  } = options;

  const zLLMJudgeResult = z.object({
    score: z.number().min(1).max(5).describe('A score from 1 to 5 rating the quality of the output'),
    reasoning: z.string().describe('Detailed explanation for the score'),
    confidence: z.number().min(0).max(1).describe('Confidence level in the evaluation (0-1)'),
    suggestions: z.string().optional().describe('Suggestions for improvement'),
  });

  return {
    name,
    description: `Enhanced AI-powered evaluation based on ${criteria}`,
    category,
    weight,
    async: true,
    score: async (input: any, output: any, expected?: any): Promise<ScorerResult> => {
      const model = modelConfig || {
        model: models.groq('qwen-qwq-32b'),
        provider: 'groq',
        modelId: 'qwen-qwq-32b',
      };

      const exampleText = includeExamples ? `
Examples of scoring:
- Score 5: Perfect command, exactly what was requested, safe and efficient
- Score 4: Good command with minor issues or suboptimal flags
- Score 3: Functional but has notable problems or missing features
- Score 2: Partially correct but significant issues
- Score 1: Wrong, dangerous, or completely inappropriate
` : '';

      const prompt = `You are an expert evaluator specializing in ${criteria}.

${exampleText}

Task/Input: ${JSON.stringify(input)}
Generated Output: ${JSON.stringify(output)}
${expected ? `Expected Output: ${JSON.stringify(expected)}` : ''}

Evaluate the output based on ${criteria}. Consider:
1. Correctness and accuracy
2. Safety and security implications
3. Efficiency and best practices
4. Completeness and appropriateness

Provide a score (1-5), detailed reasoning, confidence level, and optional suggestions.`;

      try {
        return await withRetry(
          async () => {
            const { object } = await generateObject({
              model: model.model,
              schema: zLLMJudgeResult,
              prompt,
              temperature,
            });

            // Transform 1-5 score to 0-1 scale
            const normalizedScore = (object.score - 1) / 4;

            return {
              score: normalizedScore,
              reasoning: object.reasoning,
              confidence: object.confidence,
              metadata: {
                rawScore: object.score,
                suggestions: object.suggestions,
                criteria
              }
            };
          },
          maxRetries,
          delayMs
        );
      } catch (error) {
        console.error(`Enhanced LLM Judge scoring failed: ${error}`);
        return {
          score: 0,
          reasoning: `Evaluation failed: ${error}`,
          confidence: 0,
          metadata: { error: true }
        };
      }
    },
  };
}

// Utility Functions

function calculateSimilarity(str1: string, str2: string): number {
  const longer = str1.length > str2.length ? str1 : str2;
  const shorter = str1.length > str2.length ? str2 : str1;
  
  if (longer.length === 0) return 1.0;
  
  const distance = levenshteinDistance(longer, shorter);
  return (longer.length - distance) / longer.length;
}

function levenshteinDistance(str1: string, str2: string): number {
  const matrix = Array(str2.length + 1)
    .fill(null)
    .map(() => Array(str1.length + 1).fill(null));

  for (let i = 0; i <= str1.length; i++) {
    matrix[0][i] = i;
  }

  for (let j = 0; j <= str2.length; j++) {
    matrix[j][0] = j;
  }

  for (let j = 1; j <= str2.length; j++) {
    for (let i = 1; i <= str1.length; i++) {
      const substitutionCost = str1[i - 1] === str2[j - 1] ? 0 : 1;
      matrix[j][i] = Math.min(
        matrix[j][i - 1] + 1,
        matrix[j - 1][i] + 1,
        matrix[j - 1][i - 1] + substitutionCost
      );
    }
  }

  return matrix[str2.length][str1.length];
}

async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  baseDelayMs: number = 1000
): Promise<T> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      const isRateLimitError =
        error &&
        (String(error).toLowerCase().includes('rate limit') ||
          String(error).toLowerCase().includes('too many requests'));

      if (!isRateLimitError || attempt === maxRetries - 1) {
        throw error;
      }

      const delayMs = baseDelayMs * Math.pow(2, attempt);
      console.log(`Rate limited, retrying in ${delayMs}ms (attempt ${attempt + 1}/${maxRetries})`);
      await new Promise(resolve => setTimeout(resolve, delayMs));
    }
  }
  throw new Error('Max retries exceeded');
}

// Statistics Utilities

function calculateStatistics(scores: number[]): {
  min: number;
  max: number;
  median: number;
  stdDev: number;
} {
  if (scores.length === 0) {
    return { min: 0, max: 0, median: 0, stdDev: 0 };
  }

  const sorted = [...scores].sort((a, b) => a - b);
  const min = sorted[0];
  const max = sorted[sorted.length - 1];
  const median = sorted.length % 2 === 0
    ? (sorted[sorted.length / 2 - 1] + sorted[sorted.length / 2]) / 2
    : sorted[Math.floor(sorted.length / 2)];

  const mean = scores.reduce((sum, score) => sum + score, 0) / scores.length;
  const variance = scores.reduce((sum, score) => sum + Math.pow(score - mean, 2), 0) / scores.length;
  const stdDev = Math.sqrt(variance);

  return { min, max, median, stdDev };
}

// Enhanced Evaluation Runner

export async function runEnhancedEval<TInput = any, TOutput = any, TExpected = any>(
  name: string,
  config: EvalConfig<TInput, TOutput, TExpected>
): Promise<EvalSummary> {
  const startTime = new Date();
  const { data, task, scorers, options = {} } = config;
  const {
    parallel = false,
    maxConcurrency = 3,
    saveResults = false,
    outputDir = './eval-results',
    continueOnError = true,
    timeout = 30000,
    retries = 1
  } = options;

  console.log(`üß™ Running enhanced evaluation: ${name}`);
  console.log(`üìä Scorers: ${scorers.map(s => s.name).join(', ')}`);
  console.log(`‚öôÔ∏è  Options: parallel=${parallel}, timeout=${timeout}ms, retries=${retries}\n`);

  const testData = await Promise.resolve(data());
  if (!Array.isArray(testData) || testData.length === 0) {
    throw new Error('Data function must return a non-empty array of test cases');
  }

  const results: EvalResult<TInput, TOutput, TExpected>[] = [];
  const scoreAccumulators: Record<string, number[]> = {};
  const categoryAccumulators: Record<string, number[]> = {};
  const difficultyAccumulators: Record<string, number[]> = {};

  // Initialize accumulators
  scorers.forEach(scorer => {
    scoreAccumulators[scorer.name] = [];
    if (scorer.category) {
      categoryAccumulators[scorer.category] = categoryAccumulators[scorer.category] || [];
    }
  });

  // Run evaluation for each test case
  for (let i = 0; i < testData.length; i++) {
    const testCase = testData[i];
    console.log(`üîÑ Test ${i + 1}/${testData.length}: ${JSON.stringify(testCase.input)}`);

    const testStartTime = performance.now();
    
    try {
      // Run the task with timeout
      const output = await Promise.race([
        Promise.resolve(task(testCase.input)),
        new Promise((_, reject) => 
          setTimeout(() => reject(new Error('Task timeout')), timeout)
        )
      ]) as TOutput;

      // Calculate scores (parallel or sequential)
      const scores: Record<string, ScorerResult> = {};
      
      if (parallel) {
        const scoringPromises = scorers.map(async scorer => {
          try {
            const result = await Promise.resolve(scorer.score(testCase.input, output, testCase.expected));
            return { scorer: scorer.name, result };
          } catch (error) {
            console.error(`  ${scorer.name}: ERROR - ${error}`);
            return { 
              scorer: scorer.name, 
              result: { score: 0, reasoning: `Scoring failed: ${error}`, confidence: 0 } 
            };
          }
        });

        const scoringResults = await Promise.all(scoringPromises);
        scoringResults.forEach(({ scorer, result }) => {
          scores[scorer] = result;
        });
      } else {
        for (const scorer of scorers) {
          try {
            const result = await Promise.resolve(scorer.score(testCase.input, output, testCase.expected));
            scores[scorer.name] = result;
            console.log(`  ${scorer.name}: ${result.score.toFixed(3)} (confidence: ${result.confidence?.toFixed(3) || 'N/A'})`);
          } catch (error) {
            console.error(`  ${scorer.name}: ERROR - ${error}`);
            scores[scorer.name] = { score: 0, reasoning: `Scoring failed: ${error}`, confidence: 0 };
          }
        }
      }

      // Accumulate scores
      Object.entries(scores).forEach(([scorerName, result]) => {
        scoreAccumulators[scorerName].push(result.score);
        
        const scorer = scorers.find(s => s.name === scorerName);
        if (scorer?.category) {
          categoryAccumulators[scorer.category].push(result.score);
        }
        
        if (testCase.metadata?.difficulty) {
          difficultyAccumulators[testCase.metadata.difficulty] = 
            difficultyAccumulators[testCase.metadata.difficulty] || [];
          difficultyAccumulators[testCase.metadata.difficulty].push(result.score);
        }
      });

      const executionTime = performance.now() - testStartTime;
      
      results.push({
        testCase,
        output,
        scores,
        executionTime,
        timestamp: new Date().toISOString(),
      });

    } catch (error) {
      if (!continueOnError) {
        throw error;
      }
      
      console.error(`  Task failed: ${error}`);
      const errorScores: Record<string, ScorerResult> = {};
      
      scorers.forEach(scorer => {
        errorScores[scorer.name] = { score: 0, reasoning: `Task failed: ${error}`, confidence: 0 };
        scoreAccumulators[scorer.name].push(0);
      });

      results.push({
        testCase,
        output: undefined as any,
        scores: errorScores,
        error: error instanceof Error ? error.message : String(error),
        executionTime: performance.now() - testStartTime,
        timestamp: new Date().toISOString(),
      });
    }

    console.log('');
  }

  const endTime = new Date();

  // Calculate statistics
  const averageScores: Record<string, number> = {};
  const weightedAverageScores: Record<string, number> = {};
  const statistics: Record<string, { min: number; max: number; median: number; stdDev: number }> = {};

  Object.entries(scoreAccumulators).forEach(([scorerName, scores]) => {
    averageScores[scorerName] = scores.reduce((sum, score) => sum + score, 0) / scores.length;
    statistics[scorerName] = calculateStatistics(scores);
    
    // Calculate weighted average
    const scorer = scorers.find(s => s.name === scorerName);
    const weight = scorer?.weight || 1.0;
    weightedAverageScores[scorerName] = averageScores[scorerName] * weight;
  });

  // Calculate category averages
  const categoryAverages: Record<string, number> = {};
  Object.entries(categoryAccumulators).forEach(([category, scores]) => {
    categoryAverages[category] = scores.reduce((sum, score) => sum + score, 0) / scores.length;
  });

  // Calculate difficulty averages
  const difficultyAverages: Record<string, number> = {};
  Object.entries(difficultyAccumulators).forEach(([difficulty, scores]) => {
    difficultyAverages[difficulty] = scores.reduce((sum, score) => sum + score, 0) / scores.length;
  });

  const summary: EvalSummary = {
    name,
    totalTests: testData.length,
    averageScores,
    weightedAverageScores,
    results,
    metadata: {
      startTime: startTime.toISOString(),
      endTime: endTime.toISOString(),
      duration: endTime.getTime() - startTime.getTime(),
      categories: categoryAverages,
      difficulties: difficultyAverages,
    },
    statistics: {
      min: Object.fromEntries(Object.entries(statistics).map(([k, v]) => [k, v.min])),
      max: Object.fromEntries(Object.entries(statistics).map(([k, v]) => [k, v.max])),
      median: Object.fromEntries(Object.entries(statistics).map(([k, v]) => [k, v.median])),
      stdDev: Object.fromEntries(Object.entries(statistics).map(([k, v]) => [k, v.stdDev])),
    },
  };

  // Print enhanced summary
  console.log(`üìã Enhanced Evaluation Summary: ${name}`);
  console.log(`üìä Total tests: ${testData.length}`);
  console.log(`‚è±Ô∏è  Duration: ${summary.metadata.duration}ms`);
  console.log('üìà Average scores:');
  Object.entries(averageScores).forEach(([scorer, avg]) => {
    const stats = statistics[scorer];
    console.log(`  ${scorer}: ${avg.toFixed(3)} (min: ${stats.min.toFixed(3)}, max: ${stats.max.toFixed(3)}, œÉ: ${stats.stdDev.toFixed(3)})`);
  });

  if (Object.keys(categoryAverages).length > 0) {
    console.log('üìÇ Category averages:');
    Object.entries(categoryAverages).forEach(([category, avg]) => {
      console.log(`  ${category}: ${avg.toFixed(3)}`);
    });
  }

  if (Object.keys(difficultyAverages).length > 0) {
    console.log('üéØ Difficulty averages:');
    Object.entries(difficultyAverages).forEach(([difficulty, avg]) => {
      console.log(`  ${difficulty}: ${avg.toFixed(3)}`);
    });
  }

  // Save results if requested
  if (saveResults) {
    try {
      await fs.mkdir(outputDir, { recursive: true });
      const filename = `eval-${name.replace(/\s+/g, '-')}-${Date.now()}.json`;
      const filepath = path.join(outputDir, filename);
      await fs.writeFile(filepath, JSON.stringify(summary, null, 2));
      console.log(`üíæ Results saved to: ${filepath}`);
    } catch (error) {
      console.error(`Failed to save results: ${error}`);
    }
  }

  console.log('');
  return summary;
}

export { runEnhancedEval as eval }; 